{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cir_data(data):\n",
    "    if len(data)!=SAMPLES*6*2:\n",
    "        raise BaseException('CIR data incomplete...')\n",
    "    data_array = []\n",
    "    for i in range(0, len(data), 6):\n",
    "        # [low mid high]\n",
    "        real_part = [data[i], data[i+1], data[i+2]]\n",
    "        imag_part = [data[i+3], data[i+4], data[i+5]]\n",
    "        real_num = int24to32(real_part)\n",
    "        imag_num = int24to32(imag_part)\n",
    "        data_array.append(real_num+1j*imag_num)\n",
    "    return data_array\n",
    "\n",
    "def int24to32(arr):\n",
    "    num = np.int32(0)\n",
    "    # print(f\"{arr[2]: b}\")\n",
    "    if not arr[2]&0b1000_0000:\n",
    "        # positive\n",
    "        num = ((((num|arr[2])<<8)|arr[1])<<8)|arr[0]\n",
    "    else:\n",
    "        # negative\n",
    "        # 11111111 00000000 00000000 00000000\n",
    "        # 00000000 11111111 11111111 11111111\n",
    "        # 00000001 00000000 00000000 00000000\n",
    "        # num_mask = np.int32(-2**24)\n",
    "        num = (((((num|arr[2])<<8)|arr[1])<<8)|arr[0])<<8\n",
    "        # num = num|num_mask\n",
    "        num = (~(num-1))>>8\n",
    "        num = -num\n",
    "        # -8388608\n",
    "    return num\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def fixed_point_to_decimal(fixed_point_data):\n",
    "    integer_part = fixed_point_data >> 6\n",
    "    decimal_part = (fixed_point_data & 0x3F) / (1 << 6)\n",
    "    return integer_part + decimal_part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_str = rb'''TAG_ID:0000;SEQ:52;CIR_PDOA:-10.826888,sts_Power:6,sts_F1:6639,sts_F2:6549,sts_F3:2199,sts_FpIndex:22658,sts_AccumCount:32,sts_Peak_Index:198,sts_Peak_Amp:6639,sts2_Power:9,sts2_F1:6978,sts2_F2:6497,sts2_F3:3560,sts2_FpIndex:22661,sts2_AccumCount:32,sts2_Peak_Index:198,sts2_Peak_Amp:6639,sts_quality:3,sts_quality_index:60,\n",
    "# P_FIN:-16.254320;P_POLL:-11.106652;AOA:-4.588250;DIST:440.000000;\n",
    "# CIR:123END'''\n",
    "\n",
    "# p = re.compile(b'sts_FpIndex:(.+?),.+?sts2_FpIndex:(.+?),.+?CIR:(.+?)END', flags=re.DOTALL)\n",
    "\n",
    "# match = re.findall(p, test_str)\n",
    "\n",
    "# print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CIR_data_LOS_1_1.txt', 'CIR_data_LOS_1_2.txt', 'CIR_data_LOS_2_1.txt', 'CIR_data_LOS_2_2.txt', 'CIR_data_LOS_3_1.txt', 'CIR_data_LOS_3_2.txt', 'CIR_data_LOS_4_1.txt', 'CIR_data_LOS_4_2.txt', 'CIR_data_NLOS_1_1.txt', 'CIR_data_NLOS_1_2.txt', 'CIR_data_NLOS_2_1.txt', 'CIR_data_NLOS_2_2.txt', 'CIR_data_NLOS_3_1.txt', 'CIR_data_NLOS_3_2.txt', 'CIR_data_NLOS_4_1.txt', 'CIR_data_NLOS_4_2.txt']\n"
     ]
    }
   ],
   "source": [
    "root = \"raw/\"\n",
    "files = os.listdir(root)\n",
    "\n",
    "print(files)\n",
    "\n",
    "data = []\n",
    "for file in files:\n",
    "    with open(root + file, 'rb') as f:\n",
    "        content = f.read()\n",
    "        \n",
    "        p = re.compile(b'sts_FpIndex:(.+?),.+?sts2_FpIndex:(.+?),.+?CIR:(.+?)END', flags=re.DOTALL)\n",
    "        match = re.findall(p, content)\n",
    "        processed_match = []\n",
    "        for i in range(len(match)):\n",
    "            sts1_fp_index = int(match[i][0])\n",
    "            sts2_fp_index = int(match[i][1])\n",
    "            cir = match[i][2]\n",
    "            processed_match.append([sts1_fp_index, sts2_fp_index, cir])\n",
    "        data.append(processed_match)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22658, 22661, b'\\xad\\xf2\\xff\\xff\\x0b\\x00\\x00\\xea\\xff\\xff\\x1b\\x00\\x00\\xf3\\xff\\xff\\x12\\x00\\x00\\xeb\\xff\\xff*\\x00\\x00\\x03\\x00\\x00\\x03\\x00\\x00\\xf6\\xff\\xff\\xdd\\xff\\xff\\xfe\\xff\\xff\\xfa\\xff\\xff\\xde\\xff\\xff\\xf3\\xff\\xff\\xfc\\xff\\xff\\x16\\x00\\x00\\x15\\x00\\x00\\xf5\\xff\\xff\\t\\x00\\x00\\x02\\x00\\x00\\x04\\x00\\x00\\x19\\x00\\x00\\r\\x00\\x00\\x15\\x00\\x00\\x0f\\x00\\x00\\xea\\xff\\xff\\x06\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x03\\x00\\x00\\xef\\xff\\xff\\x07\\x00\\x00\\xf7\\xff\\xff\\x06\\x00\\x00\\xfe\\xff\\xff\\x05\\x00\\x00\\x01\\x00\\x00\\x18\\x00\\x00\\x04\\x00\\x00\\xf4\\xff\\xff\\xdd\\xff\\xff\\xf6\\xff\\xff\\xdb\\xff\\xff\\xe8\\xff\\xff\\x03\\x00\\x00\\x12\\x00\\x00\\xf0\\xff\\xff\\xf6\\xff\\xff\\x0c\\x00\\x00\\x12\\x00\\x00\\x02\\x00\\x00\\xf8\\xff\\xff\\x10\\x00\\x00\\x1d\\x00\\x00\\xf4\\xff\\xff\\xfa\\xff\\xff\\xfe\\xff\\xff\\x12\\x00\\x00\\x05\\x00\\x00\\n\\x00\\x00\\x0f\\x00\\x00\\xfd\\xff\\xff1\\x00\\x00\\xef\\xff\\xff.\\x00\\x00\\xef\\xff\\xff*\\x00\\x00\\xe1\\xff\\xff\\x15\\x00\\x00\\x18\\x00\\x00\\r\\x00\\x00\\x15\\x00\\x00\\x0c\\x00\\x007\\x00\\x00\\xff\\xff\\xff\\x11\\x00\\x00\\xe7\\xff\\xff\\x07\\x00\\x00\\xf4\\xff\\xff\\xe5\\xff\\xff\\xf3\\xff\\xff\\xe1\\xff\\xff\\x01\\x00\\x00\\xf8\\xff\\xff\\xe9\\xff\\xff\\xf1\\xff\\xff\\xef\\xff\\xff\\t\\x00\\x00\\x10\\x00\\x00\\r\\x00\\x00\\xf8\\xff\\xff\\x17\\x00\\x00\\x01\\x00\\x00\\xfb\\xff\\xff\\x06\\x00\\x00\\x1a\\x00\\x00\\x10\\x00\\x00\\x10\\x00\\x00\\x07\\x00\\x00\\xee\\xff\\xff\\t\\x00\\x00\\x01\\x00\\x00\\x04\\x00\\x00\\x01\\x00\\x00\\x07\\x00\\x00!\\x00\\x00(\\x00\\x00#\\x00\\x00\\x0e\\x00\\x00\\t\\x00\\x00\\xf4\\xff\\xff\\xfc\\xff\\xff\\xf8\\xff\\xff\\xe9\\xff\\xff\\xf6\\xff\\xff\\xd6\\xff\\xff\\xec\\xff\\xff\\xe8\\xff\\xff\\xf9\\xff\\xff\\xfe\\xff\\xff\\x12\\x00\\x00\\xfd\\xff\\xff\\x0b\\x00\\x00*\\x00\\x00\\xfe\\xff\\xff\\x18\\x00\\x00\\x13\\x00\\x00\\x13\\x00\\x00\\xf8\\xff\\xff\\xf8\\xff\\xff\\xe0\\xff\\xff\\xf1\\xff\\xff\\x06\\x00\\x00\\x00\\x00\\x00\\x19\\x00\\x00\\x06\\x00\\x00.\\x00\\x00\\n\\x00\\x00\\x05\\x00\\x00\\xf4\\xff\\xff\\x04\\x00\\x00\\xfb\\xff\\xff\\xf9\\xff\\xff\\x08\\x00\\x00\\xfa\\xff\\xff\\x01\\x00\\x00\\x0b\\x00\\x00\\x03\\x00\\x00\\x1d\\x00\\x00\\xf1\\xff\\xff\\x16\\x00\\x00\\x0b\\x00\\x00\\r\\x00\\x00\\x06\\x00\\x00\\x1e\\x00\\x00\\x0b\\x00\\x00\\x06\\x00\\x00\\xf5\\xff\\xff\\xfc\\xff\\xff\\xee\\xff\\xff\\xfe\\xff\\xff\\xfc\\xff\\xff\\x11\\x00\\x00\\x14\\x00\\x00\\xff\\xff\\xff\\xf7\\xff\\xff\\xfd\\xff\\xff\\x1a\\x00\\x00\\xff\\xff\\xff\\xe9\\xff\\xff\\xfc\\xff\\xff\\xe8\\xff\\xff\\n\\x00\\x00\\xd4\\xff\\xff\\x1b\\x00\\x00\\x15\\x00\\x00\\xf0\\xff\\xff\\xfb\\xff\\xff\\xfd\\xff\\xff\\x0f\\x00\\x00\\xe1\\xff\\xff\\xf0\\xff\\xff\\xf0\\xff\\xff\\x0b\\x00\\x00\\xf8\\xff\\xff\\x11\\x00\\x00\\xf2\\xff\\xff\\x10\\x00\\x00\\xe7\\xff\\xff\\'\\x00\\x00\\xda\\xff\\xff\\x1b\\x00\\x00\\xdc\\xff\\xffE\\x00\\x00\\xe3\\xff\\xff\\\\\\x00\\x00\\x02\\x00\\x00$\\x00\\x00\\xf4\\xff\\xff\\xdd\\xff\\xff\\xd6\\xff\\xff\\xbf\\xff\\xff\\xe1\\xff\\xff\\xdd\\xff\\xff\\xf4\\xff\\xff\\x00\\x00\\x00\\xf9\\xff\\xff\\x0c\\x00\\x00\\x0e\\x00\\x00!\\x00\\x00B\\x00\\x00\\x1b\\x00\\x00(\\x00\\x00\\x03\\x00\\x00\\x04\\x00\\x00\\xf3\\xff\\xff\\xf9\\xff\\xff\\xd1\\xff\\xff\\x03\\x00\\x00\\xe6\\xff\\xff\\xee\\xff\\xff\\t\\x00\\x00\\x18\\x00\\x00$\\x00\\x00\\xf6\\xff\\xffN\\x00\\x00\\xe9\\xff\\xff/\\x00\\x00\\xf5\\xff\\xff\\xfd\\xff\\xff\\xed\\xff\\xff\\r\\x00\\x00\\xd9\\xff\\xff\\x05\\x00\\x00\\xda\\xff\\xff\\xfe\\xff\\xff\\xf7\\xff\\xff\\xeb\\xff\\xff\\xda\\xff\\xff\\xec\\xff\\xff\\x12\\x00\\x00\\x05\\x00\\x00\\x19\\x00\\x002\\x00\\x00\\'\\x00\\x00\\x1e\\x00\\x00\\x11\\x00\\x00\\x02\\x00\\x00\\n\\x00\\x00\\x01\\x00\\x00&\\x00\\x00\\xef\\xff\\xff\\x13\\x00\\x00\\xeb\\xff\\xff\\xe8\\xff\\xff\\x11\\x00\\x00\\xe7\\xff\\xff\\xef\\xff\\xff\\x00\\x00\\x00\\xee\\xff\\xff\\x00\\x00\\x00\\x02\\x00\\x00\\xfd\\xff\\xff\\r\\x00\\x00\\xfa\\xff\\xff\"\\x00\\x00\\xe8\\xff\\xff,\\x00\\x00\\xf1\\xff\\xff\\x0c\\x00\\x00\\x0c\\x00\\x00\\x05\\x00\\x00\\xfd\\xff\\xff\\x01\\x00\\x00\\xec\\xff\\xff\\x01\\x00\\x00\\xf7\\xff\\xff\\xf5\\xff\\xff\\xf7\\xff\\xff\\x00\\x00\\x00\\x0c\\x00\\x00\\x14\\x00\\x00\\t\\x00\\x00\\x1a\\x00\\x00\\xf0\\xff\\xff\\x10\\x00\\x00\\xda\\xff\\xff\\xf5\\xff\\xff\\xf2\\xff\\xff\\xfa\\xff\\xff\\x1c\\x00\\x00\\xef\\xff\\xff%\\x00\\x00\\xf2\\xff\\xff\\xf4\\xff\\xff\\r\\x00\\x00\\xfd\\xff\\xff\\x03\\x00\\x00\\x08\\x00\\x00\\x06\\x00\\x00\\x05\\x00\\x00\\x0f\\x00\\x00\\x15\\x00\\x00\\x0b\\x00\\x00\\x0b\\x00\\x00\\x02\\x00\\x00\\xf2\\xff\\xff\\xde\\xff\\xff\\x03\\x00\\x00\\xf1\\xff\\xff\\x00\\x00\\x00\\x04\\x00\\x00\\x05\\x00\\x00\\xfc\\xff\\xff\\xfa\\xff\\xff\\x05\\x00\\x00\\x12\\x00\\x00\\xfc\\xff\\xff\\x11\\x00\\x00\\xef\\xff\\xff-\\x00\\x00\\xd4\\xff\\xff\"\\x00\\x00\\xb0\\xff\\xff\\n\\x00\\x00\\xe8\\xff\\xff#\\x00\\x00\\x1e\\x00\\x00\\x19\\x00\\x00-\\x00\\x00\\x0b\\x00\\x00\\x10\\x00\\x00\\x06\\x00\\x00\\x08\\x00\\x00\\r\\x00\\x00\\x03\\x00\\x00\\x02\\x00\\x00\\xe7\\xff\\xff\\xf5\\xff\\xff\\xed\\xff\\xff\\xf9\\xff\\xff\\xea\\xff\\xff\\xfd\\xff\\xff\\xfc\\xff\\xff\\xf2\\xff\\xff\\xe7\\xff\\xff\\xec\\xff\\xff\\x04\\x00\\x00\\x02\\x00\\x00\\x12\\x00\\x00\\x00\\x00\\x00\\x0b\\x00\\x00-\\x00\\x00\\xe6\\xff\\xff\\x15\\x00\\x00\\xec\\xff\\xff\\x1a\\x00\\x00\\xff\\xff\\xff)\\x00\\x004\\x00\\x00%\\x00\\x00\\x1d\\x00\\x00(\\x00\\x00\\x0b\\x00\\x00\\xfa\\xff\\xff\\x03\\x00\\x00\\t\\x00\\x00\\xeb\\xff\\xff\\xdd\\xff\\xff\\xcc\\xff\\xff\\xf6\\xff\\xff\\xc7\\xff\\xff\\xea\\xff\\xff\\xef\\xff\\xff\\xf9\\xff\\xff\\x02\\x00\\x00\\xff\\xff\\xff\\x10\\x00\\x00\\x04\\x00\\x00\\xed\\xff\\xff\\xed\\xff\\xff\\x0b\\x00\\x00\\t\\x00\\x00\\x02\\x00\\x00\\x0c\\x00\\x00\\xe5\\xff\\xff$\\x00\\x00\\xf3\\xff\\xff\\x0f\\x00\\x00\\x01\\x00\\x00\\x12\\x00\\x00)\\x00\\x00\\x16\\x00\\x00!\\x00\\x00%\\x00\\x00\\xfa\\xff\\xff\\x03\\x00\\x00\\xf9\\xff\\xff\\x05\\x00\\x00\\x05\\x00\\x00\\x04\\x00\\x00\\x06\\x00\\x00\\xec\\xff\\xff\\x07\\x00\\x00\\xf1\\xff\\xff\\xe1\\xff\\xff\\x11\\x00\\x00\\xe2\\xff\\xff\\x1e\\x00\\x00\\x08\\x00\\x00\\x01\\x00\\x00!\\x00\\x00\\xf0\\xff\\xff\\x0e\\x00\\x00\\xd8\\xff\\xff\\x1e\\x00\\x00\\xe4\\xff\\xff\\x18\\x00\\x00\\xf8\\xff\\xff\\x1a\\x00\\x00\\x0e\\x00\\x00\\x16\\x00\\x00\\x1f\\x00\\x00\\x08\\x00\\x00\\xf5\\xff\\xff\\x08\\x00\\x00\\xf9\\xff\\xff\\x12\\x00\\x00\\xf1\\xff\\xff\\x02\\x00\\x00\\xfb\\xff\\xff\\x02\\x00\\x00\\x08\\x00\\x00\\xfe\\xff\\xff\\x15\\x00\\x00\\x11\\x00\\x00\\x1c\\x00\\x00\\xf1\\xff\\xff\\xfc\\xff\\xff\\x00\\x00\\x00\\r\\x00\\x00\\x08\\x00\\x00\\xfe\\xff\\xff\\x07\\x00\\x00\\x16\\x00\\x00\\x07\\x00\\x00\\x05\\x00\\x00\\x13\\x00\\x00\\x00\\x00\\x00\"\\x00\\x00\\xf1\\xff\\xff1\\x00\\x00\\xf2\\xff\\xff\"\\x00\\x00\\xe9\\xff\\xff\\xf5\\xff\\xff\\xca\\xff\\xff\\xed\\xff\\xff\\xe4\\xff\\xff\\xfa\\xff\\xff\\xef\\xff\\xff\\x1b\\x00\\x00\\xfb\\xff\\xff\\x1a\\x00\\x00\\xfa\\xff\\xff<\\x00\\x00\\x19\\x00\\x003\\x00\\x00\\xfc\\xff\\xff5\\x00\\x00\\xff\\xff\\xff\\x07\\x00\\x00\\xfd\\xff\\xff\\xd8\\xff\\xff\\x01\\x00\\x00\\xe9\\xff\\xff\\xe0\\xff\\xff#\\x00\\x00\\xc3\\xff\\xff5\\x00\\x00\\xdf\\xff\\xff+\\x00\\x00\\xf1\\xff\\xff \\x00\\x00\\x0b\\x00\\x00\\x01\\x00\\x00\\xe1\\xff\\xff\\xe8\\xff\\xff\\xdb\\xff\\xff\\xd2\\xff\\xff\\xf0\\xff\\xff\\xef\\xff\\xff\\x0c\\x00\\x00\\xff\\xff\\xff\\xfb\\xff\\xff\\xf8\\xff\\xff\\x08\\x00\\x00\\xfd\\xff\\xff\\x03\\x00\\x00\\x18\\x00\\x00\\x06\\x00\\x00\\xfe\\xff\\xff\\x15\\x00\\x00\\x06\\x00\\x00\\x0b\\x00\\x00\\x13\\x00\\x00\\x11\\x00\\x00\\x01\\x00\\x00\\x12\\x00\\x00 \\x00\\x00\\x0e\\x00\\x00\\x11\\x00\\x00\\xf9\\xff\\xff\\x11\\x00\\x00\\xe8\\xff\\xff\\x0e\\x00\\x00\\xe1\\xff\\xff\\xfd\\xff\\xff\\xc4\\xff\\xff\\xe0\\xff\\xff\\xd6\\xff\\xff\\xf0\\xff\\xff\\xd5\\xff\\xff\\xfa\\xff\\xff\\xfe\\xff\\xff\\x03\\x00\\x00\\xf0\\xff\\xff\\xe0\\xff\\xff\\t\\x00\\x00\\xef\\xff\\xff\\x17\\x00\\x00\\x07\\x00\\x00\\x0c\\x00\\x00\\x13\\x00\\x00\\x00\\x00\\x00\\x14\\x00\\x00\\x01\\x00\\x00\\x02\\x00\\x00\\x06\\x00\\x00\\x08\\x00\\x00\\x03\\x00\\x00\\r\\x00\\x00\\xf4\\xff\\xff\\x18\\x00\\x00\\xfd\\xff\\xff\\xf8\\xff\\xff\\xf5\\xff\\xff\\xe0\\xff\\xff\\xf0\\xff\\xff\\xe2\\xff\\xff\\x0c\\x00\\x00\\x04\\x00\\x00\\xfb\\xff\\xff\\x1b\\x00\\x00\\n\\x00\\x00\\x17\\x00\\x00\\n\\x00\\x00\\x07\\x00\\x00\\x00\\x00\\x00\\xea\\xff\\xff\\xf5\\xff\\xff\\xee\\xff\\xff\\xe2\\xff\\xff\\xfc\\xff\\xff\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x0c\\x00\\x00\\xf4\\xff\\xff\\xfc\\xff\\xff\\xfb\\xff\\xff\\x00\\x00\\x00\\r\\x00\\x00\\xfd\\xff\\xff\\x19\\x00\\x00\\xf2\\xff\\xff\\x11\\x00\\x00\\xef\\xff\\xff\\x1a\\x00\\x00\\x01\\x00\\x00\\xfc\\xff\\xff\\xf9\\xff\\xff\\x05\\x00\\x00\\xdb\\xff\\xff\\x18\\x00\\x00\\xe6\\xff\\xff\\xea\\xff\\xff\\x0e\\x00\\x00\\xf8\\xff\\xff\\t\\x00\\x00\\x0e\\x00\\x00\\x14\\x00\\x00\\x06\\x00\\x00\"\\x00\\x00\\xed\\xff\\xff\\x11\\x00\\x00\\x01\\x00\\x00\\x0e\\x00\\x00\\x1f\\x00\\x00\\x01\\x00\\x00\\x0f\\x00\\x00\\x03\\x00\\x00\\x06\\x00\\x00\\x15\\x00\\x00\\x0c\\x00\\x00\\n\\x00\\x00\\xf6\\xff\\xff\\r\\x00\\x00\\xf3\\xff\\xff0\\x00\\x00\\x02\\x00\\x00\\x18\\x00\\x00\\x04\\x00\\x00\\xfc\\xff\\xff\\xf8\\xff\\xff\\x13\\x00\\x00\\xea\\xff\\xff\\xfc\\xff\\xff\\xe4\\xff\\xff\\x17\\x00\\x00\\xfc\\xff\\xff\\xf2\\xff\\xff\\xfa\\xff\\xff\\x1a\\x00\\x00\\x1c\\x00\\x00\\x18\\x00\\x00?\\x00\\x00\\x17\\x00\\x00 \\x00\\x00\\xfe\\xff\\xff\\x1e\\x00\\x00\\xf3\\xff\\xff\\x0e\\x00\\x00\\x05\\x00\\x00\\x0f\\x00\\x00\\xfd\\xff\\xff\\x02\\x00\\x00\\xee\\xff\\xff\\x0c\\x00\\x00\\xfb\\xff\\xff\\x11\\x00\\x00\\xfe\\xff\\xff\\x10\\x00\\x00\\t\\x00\\x00\\xee\\xff\\xff\\x1b\\x00\\x00\\xce\\xff\\xff\\x0e\\x00\\x00\\xdd\\xff\\xff\\x18\\x00\\x00\\xff\\xff\\xff\\x18\\x00\\x00\\xf3\\xff\\xff\\x03\\x00\\x00\\xfe\\xff\\xff\\xf2\\xff\\xff\\xfe\\xff\\xff\\xe7\\xff\\xff\\xe6\\xff\\xff\\xf5\\xff\\xff\\xe7\\xff\\xff\\xe1\\xff\\xff\\n\\x00\\x00\\x1b\\x00\\x00\\x04\\x00\\x00\\x15\\x00\\x00\\x18\\x00\\x00\\xfc\\xff\\xff\\x03\\x00\\x00\\x01\\x00\\x00\\xfe\\xff\\xff\\xfc\\xff\\xff\\xfd\\xff\\xff\\x13\\x00\\x00\\x10\\x00\\x00.\\x00\\x00\\xf3\\xff\\xff\\n\\x00\\x00\\xfc\\xff\\xff\\x02\\x00\\x00\\xfb\\xff\\xff\\xd5\\xff\\xff\\xed\\xff\\xff\\xd7\\xff\\xff\\xfb\\xff\\xff\\x05\\x00\\x00\\xfc\\xff\\xff\\xfa\\xff\\xff\\xfd\\xff\\xff\\x11\\x00\\x00\\xfd\\xff\\xff\\x06\\x00\\x00\\x1c\\x00\\x00\\x16\\x00\\x00\\x04\\x00\\x00\\xf3\\xff\\xff\\x07\\x00\\x00\\xef\\xff\\xff\\x1b\\x00\\x00\\xfa\\xff\\xff\\xa1\\x00\\x00\\x9f\\xff\\xff\\x8e\\x02\\x00\\xd3\\xfd\\xff\\xf5\\x03\\x00V\\xfb\\xff\\x14\\x03\\x00\\x04\\xfb\\xff9\\x01\\x00\\xec\\xfe\\xff\\xe3\\x01\\x00\\xbf\\x02\\x00\\xe2\\x02\\x00b\\x04\\x00W\\x02\\x006\\x02\\x00|\\x00\\x00\\xb8\\xff\\xff\\xaa\\xff\\xffv\\xfe\\xff\\xfe\\xfe\\xffu\\xfd\\xff\\x80\\xfe\\xff\\xa4\\xfc\\xff\\xae\\xfe\\xff6\\xfd\\xff\\x84\\xff\\xff\\x83\\xff\\xff\\xf2\\xff\\xff\\x93\\x01\\x00\\x01\\x00\\x00\\xcf\\x01\\x00\\xd4\\xff\\xff\\x9a\\x00\\x00J\\x00\\x00Q\\xff\\xff\\xb7\\x00\\x00\\x08\\xff\\xff*\\x01\\x00\\xd3\\xff\\xff\\xf0\\x00\\x00\\x9b\\x00\\x00\\xde\\x01\\x00\\x9d\\x01\\x00\\x82\\x02\\x00\\x1e\\x01\\x00s\\x02\\x00\\xd7\\x00\\x00\\xc0\\x00\\x00\\xa1\\x00\\x00\\xdf\\xff\\xffE\\x00\\x00\\xd7\\xfe\\xff\\xb4\\xfe\\xff\\xcc\\xfe\\xff\\xfd\\xfd\\xffP\\xff\\xff\\x80\\xff\\xff\\xb1\\xff\\xff\\x90\\x00\\x00y\\xff\\xff\\xcb\\x00\\x00\\x82\\xff\\xff=\\x00\\x00\\xfc\\xff\\xff\\xb7\\xff\\xff\\xe0\\x00\\x00K\\xff\\xff\\x9a\\x00\\x00x\\xff\\xff\\x13\\x00\\x00\\xce\\xff\\xff\\xd1\\xff\\xff\\x1f\\x00\\x00\\xef\\xff\\xff\\x1b\\x00\\x00\\x1b\\x00\\x00\\xe6\\xff\\xff+\\x00\\x00\\xa2\\xff\\xff\\xfa\\xff\\xff\\xa8\\xff\\xff\\xb8\\xff\\xff\\xd8\\xff\\xff\\xd3\\xff\\xffS\\x00\\x00o\\x00\\x00~\\x00\\x00\\xf5\\x00\\x009\\x00\\x00y\\x00\\x00\\xcd\\xff\\xff-\\x00\\x00\\x98\\xff\\xff\\xd4\\xff\\xff\\xdc\\xff\\xff\\xb3\\xff\\xff%\\x00\\x00S\\xff\\xff\\xa1\\x00\\x00\\x9e\\xff\\xff\\xb1\\x00\\x00\\x92\\xff\\xff\\xc9\\x00\\x00\\x9a\\xff\\xffp\\x00\\x00\\xb9\\xff\\xff&\\x00\\x00\\xdf\\xff\\xff\\xd8\\xff\\xff\\xd6\\xff\\xff\\xc6\\xff\\xff\\xf4\\xff\\xff\\x04\\x00\\x005\\x00\\x00]\\x00\\x00Z\\x00\\x00\\\\\\x00\\x00\\r\\x00\\x008\\x00\\x00l\\xff\\xff\\xd1\\xff\\xff\\x9e\\xff\\xff\\xcb\\xff\\xff\\xef\\xff\\xff\\xb5\\xff\\xff$\\x00\\x00\\xce\\xff\\xff\\xec\\xff\\xff+\\x00\\x00f\\xff\\xff\\x89\\x00\\x00\\x91\\xff\\xff~\\x00\\x00\\xb0\\xff\\xff&\\x00\\x00\\xe4\\xff\\xff\\xf8\\xff\\xff\\xd2\\xff\\xff\\xaa\\xff\\xff\\xe2\\xff\\xff\\xf3\\xff\\xff\\x1a\\x00\\x004\\x00\\x00\\x19\\x00\\x00;\\x00\\x00!\\x00\\x00C\\x00\\x00\\x16\\x00\\x00^\\x00\\x00\\x16\\x00\\x00#\\x00\\x00!\\x00\\x00\\xd5\\xff\\xff\\x10\\x00\\x00\\xd8\\xff\\xff\\xfa\\xff\\xff\\xd8\\xff\\xff\\x0f\\x00\\x00\\x05\\x00\\x00,\\x00\\x00\\x11\\x00\\x00\\x1c\\x00\\x00+\\x00\\x00\\xfd\\xff\\xff\\xfc\\xff\\xff\\xf6\\xff\\xff\\xf0\\xff\\xff\\x00\\x00\\x00\\r\\x00\\x00\\x02\\x00\\x00=\\x00\\x00\\xf2\\xff\\xff0\\x00\\x00\\xf0\\xff\\xff\\x08\\x00\\x00\\xf8\\xff\\xff\\x17\\x00\\x00\\xd4\\xff\\xff\\x11\\x00\\x00\\xc0\\xff\\xff4\\x00\\x00\\xc3\\xff\\xff,\\x00\\x00\\xcd\\xff\\xff&\\x00\\x00\\xcd\\xff\\xff\\x13\\x00\\x00\\xee\\xff\\xff-\\x00\\x00\\x13\\x00\\x00:\\x00\\x00#\\x00\\x00\\xfc\\xff\\xff5\\x00\\x00\\xe9\\xff\\xff3\\x00\\x00\"\\x00\\x00\\xfd\\xff\\xff\\x15\\x00\\x00\\xf5\\xff\\xff\\x15\\x00\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\xee\\xff\\xff&\\x00\\x00\\xe1\\xff\\xff\\x13\\x00\\x00\\xec\\xff\\xff\\xee\\xff\\xff\\xfd\\xff\\xff\\xf5\\xff\\xff\\xf9\\xff\\xff\\x0f\\x00\\x00\\xf2\\xff\\xff\\xf1\\xff\\xff\\xf6\\xff\\xff\\x08\\x00\\x00\\x04\\x00\\x00\\x01\\x00\\x00\\x05\\x00\\x00\\xfe\\xff\\xff\\xcf\\xff\\xff\\xfa\\xff\\xff\\xc9\\xff\\xff\\x05\\x00\\x00\\xcc\\xff\\xff\\x17\\x00\\x00\\xd3\\xff\\xff5\\x00\\x00\\xd3\\xff\\xff)\\x00\\x00\\xcd\\xff\\xff\\xf2\\xff\\xff\\xb2\\xff\\xff\\xc7\\xff\\xff\\xcf\\xff\\xff\\xd6\\xff\\xff\\xda\\xff\\xff\\x13\\x00\\x00\\xfe\\xff\\xff\\x19\\x00\\x00\\xf7\\xff\\xff\\x11\\x00\\x00\\xee\\xff\\xff \\x00\\x00\\xe8\\xff\\xff)\\x00\\x00\\xe6\\xff\\xff\\x10\\x00\\x00\\xff\\xff\\xff\\xfa\\xff\\xff\\r\\x00\\x00\\xea\\xff\\xff\\x15\\x00\\x00\\xed\\xff\\xff\\x0b\\x00\\x00\\xff\\xff\\xff\\xdf\\xff\\xff(\\x00\\x00\\xc3\\xff\\xff=\\x00\\x00\\xdb\\xff\\xffP\\x00\\x00\\xcc\\xff\\xff\\x16\\x00\\x00\\xdb\\xff\\xff\\xc5\\xff\\xff\\xbe\\xff\\xff\\xca\\xff\\xff\\xce\\xff\\xff\\xf2\\xff\\xff\\xf6\\xff\\xff\\x01\\x00\\x00\\xf6\\xff\\xff&\\x00\\x00\\x1c\\x00\\x00)\\x00\\x00\\n\\x00\\x00K\\x00\\x00\\xf6\\xff\\xff8\\x00\\x00\\xf5\\xff\\xff\\x05\\x00\\x00\\xf5\\xff\\xff\\n\\x00\\x00\\xfc\\xff\\xff\\xf8\\xff\\xff\\xef\\xff\\xff\\x00\\x00\\x00\\xd9\\xff\\xff\\t\\x00\\x00\\xc4\\xff\\xff\\x15\\x00\\x00\\xd4\\xff\\xff/\\x00\\x00\\xe4\\xff\\xff\\x1a\\x00\\x00\\xe7\\xff\\xff\\x0c\\x00\\x00\\xe9\\xff\\xff\\xf6\\xff\\xff\\xd5\\xff\\xff\\x05\\x00\\x00\\xf6\\xff\\xff\\x0b\\x00\\x00\\x18\\x00\\x00\\xea\\xff\\xff(\\x00\\x00\\x0b\\x00\\x00 \\x00\\x00.\\x00\\x00\\x0f\\x00\\x00\\x11\\x00\\x00\\xfa\\xff\\xff\\x07\\x00\\x00\\xfc\\xff\\xff\\xfc\\xff\\xff\\x03\\x00\\x00\\xf5\\xff\\xff\\x03\\x00\\x00\\xfc\\xff\\xff\\xf3\\xff\\xff\\xf2\\xff\\xff\\x0f\\x00\\x00\\xf8\\xff\\xff\\x0e\\x00\\x00\\xff\\xff\\xff\\x12\\x00\\x00\\xed\\xff\\xff\\xfa\\xff\\xff\\xec\\xff\\xff\\xf2\\xff\\xff\\x1b\\x00\\x00\\xf8\\xff\\xff\\x19\\x00\\x00\\r\\x00\\x00\\x01\\x00\\x00\\x10\\x00\\x00\\xef\\xff\\xff\\t\\x00\\x00\\xd4\\xff\\xff\\x0e\\x00\\x00\\xc5\\xff\\xff\\n\\x00\\x00\\xe8\\xff\\xff\\xf2\\xff\\xff\\x0b\\x00\\x00\\xf5\\xff\\xff\\x1a\\x00\\x00\\x15\\x00\\x00\\x10\\x00\\x00\\x15\\x00\\x00\\x14\\x00\\x00!\\x00\\x00\\x14\\x00\\x00\\xe4\\xff\\xff\\x15\\x00\\x00\\xcd\\xff\\xff\\x1f\\x00\\x00\\xf4\\xff\\xff\\n\\x00\\x00)\\x00\\x00\\x06\\x00\\x00\\x10\\x00\\x00\\x14\\x00\\x00\\x04\\x00\\x00\\xf8\\xff\\xff\\x03\\x00\\x00\\xe6\\xff\\xff\\xfd\\xff\\xff\\x08\\x00\\x00\\x13\\x00\\x00\\x14\\x00\\x00\\x12\\x00\\x00\\x1f\\x00\\x00\\x1c\\x00\\x00\\x0f\\x00\\x00\\'\\x00\\x00\\x1d\\x00\\x00\\x1d\\x00\\x00\\xfb\\xff\\xff\\xf4\\xff\\xff!\\x00\\x00\\xe8\\xff\\xff\\x12\\x00\\x00\\xfe\\xff\\xff\\r\\x00\\x00\\xef\\xff\\xff\\x11\\x00\\x00\\x08\\x00\\x00\\x0f\\x00\\x00\\x17\\x00\\x00\\x1e\\x00\\x007\\x00\\x00\\x1c\\x00\\x001\\x00\\x00\\xe8\\xff\\xff;\\x00\\x00\\xfd\\xff\\xff\\x0e\\x00\\x00\\r\\x00\\x00\\xfc\\xff\\xff\\r\\x00\\x00\\xe9\\xff\\xff\\x1a\\x00\\x00\\xfc\\xff\\xff\\x11\\x00\\x00\\x06\\x00\\x00!\\x00\\x00\\x16\\x00\\x00\\x08\\x00\\x00-\\x00\\x00\\xf4\\xff\\xff\\x10\\x00\\x00\\xfe\\xff\\xff\\x15\\x00\\x00\\x11\\x00\\x00\\x04\\x00\\x00\\x03\\x00\\x00\\xf5\\xff\\xff\\xfc\\xff\\xff\\x02\\x00\\x00\\xf8\\xff\\xff\\x0b\\x00\\x00\\xda\\xff\\xff\\x15\\x00\\x00\\xf9\\xff\\xff\\x0e\\x00\\x00\\x02\\x00\\x00\\x1e\\x00\\x00\\xf1\\xff\\xff\\x17\\x00\\x00\\t\\x00\\x00\\x00\\x00\\x00\\x07\\x00\\x00\\xff\\xff\\xff\\xfc\\xff\\xff\\xe3\\xff\\xff\\x17\\x00\\x00\\xd5\\xff\\xff\\x16\\x00\\x00\\xfe\\xff\\xff\\x15\\x00\\x00\\x05\\x00\\x00\\x07\\x00\\x00\\x1e\\x00\\x00\\x06\\x00\\x00\\x1b\\x00\\x00\\xfa\\xff\\xff\\x13\\x00\\x00\\x0b\\x00\\x00\\xff\\xff\\xff\\x01\\x00\\x00\\x02\\x00\\x00\\xff\\xff\\xff\\x0b\\x00\\x00\\x15\\x00\\x00)\\x00\\x00\\xe5\\xff\\xff4\\x00\\x00\\x02\\x00\\x00:\\x00\\x00\\x05\\x00\\x00\\x12\\x00\\x00\\xf5\\xff\\xff\\xfc\\xff\\xff\\xf7\\xff\\xff\\xe1\\xff\\xff!\\x00\\x00\\xef\\xff\\xff\\x1c\\x00\\x00\\xea\\xff\\xff.\\x00\\x00\\xf9\\xff\\xff\\x0b\\x00\\x00\\x08\\x00\\x00\\xe6\\xff\\xff\\x06\\x00\\x00\\xde\\xff\\xff\\x00\\x00\\x00\\n\\x00\\x00\\x11\\x00\\x00\"\\x00\\x00$\\x00\\x00\\x12\\x00\\x00\\x13\\x00\\x00\\x0c\\x00\\x00\\xd6\\xff\\xff\\xf4\\xff\\xff\\xec\\xff\\xff\\xf4\\xff\\xff\\x01\\x00\\x00\\x01\\x00\\x00\\x08\\x00\\x00\\x12\\x00\\x00\\x0e\\x00\\x00\\xfd\\xff\\xff\\xf7\\xff\\xff\\x11\\x00\\x00\\x01\\x00\\x00\\x13\\x00\\x00\\x12\\x00\\x00\\x03\\x00\\x00\\x08\\x00\\x00\\x0f\\x00\\x00\\xda\\xff\\xff\\xf1\\xff\\xff\\xf9\\xff\\xff\\xeb\\xff\\xff\\xfb\\xff\\xff\\x00\\x00\\x00\\x17\\x00\\x00\\xf0\\xff\\xff<\\x00\\x00\\xf6\\xff\\xff\\x1d\\x00\\x00\\xea\\xff\\xff\\x16\\x00\\x00\\xe9\\xff\\xff\\xff\\xff\\xff\\xe9\\xff\\xff\\xd5\\xff\\xff\\r\\x00\\x00\\xc8\\xff\\xff\\xfc\\xff\\xff\\xdd\\xff\\xff\\x08\\x00\\x00\\xe7\\xff\\xff\\x00\\x00\\x00\\xea\\xff\\xff\\xf1\\xff\\xff\\xfd\\xff\\xff\\xeb\\xff\\xff\\x15\\x00\\x00\\xca\\xff\\xff\\x05\\x00\\x00\\xc3\\xff\\xff\\xf9\\xff\\xff\\x08\\x00\\x00\\x01\\x00\\x006\\x00\\x00 \\x00\\x00E\\x00\\x00\\x07\\x00\\x00\\x17\\x00\\x00\\xfa\\xff\\xff\\xe6\\xff\\xff\\r\\x00\\x00\\xed\\xff\\xff\\xfa\\xff\\xff\\xfd\\xff\\xff\\x08\\x00\\x00\\xf2\\xff\\xff\\x0f\\x00\\x00 \\x00\\x00\\xf2\\xff\\xff\\xfd\\xff\\xff\\x18\\x00\\x00\\xe8\\xff\\xff\\x13\\x00\\x00\\xf3\\xff\\xff\\x1f\\x00\\x00\\xfa\\xff\\xff\\x16\\x00\\x00\\x04\\x00\\x00\\xfd\\xff\\xff\\xe1\\xff\\xff\\xfc\\xff\\xff\\xef\\xff\\xff\\x08\\x00\\x00\\r\\x00\\x00\\x14\\x00\\x00\\x17\\x00\\x00\\x00\\x00\\x00\\x16\\x00\\x00\\xee\\xff\\xff\\xf8\\xff\\xff\\x05\\x00\\x00\\x12\\x00\\x00$\\x00\\x00\\x1d\\x00\\x00\\x07\\x00\\x00\\x17\\x00\\x00\\xf7\\xff\\xff\\x01\\x00\\x00\\xf6\\xff\\xff\\x05\\x00\\x00\\xfc\\xff\\xff\\x07\\x00\\x005\\x00\\x00\\x0e\\x00\\x00\\n\\x00\\x00 \\x00\\x00\\x12\\x00\\x00\\x1b\\x00\\x00\\x14\\x00\\x00 \\x00\\x00 \\x00\\x00\\x0e\\x00\\x00\\x1e\\x00\\x00\\x1a\\x00\\x00\"\\x00\\x00\\x0c\\x00\\x00\\n\\x00\\x00\\xfd\\xff\\xff\\x04\\x00\\x00\\xeb\\xff\\xff\\xef\\xff\\xff\\x01\\x00\\x00\\x11\\x00\\x00\\xf0\\xff\\xff\\xfb\\xff\\xff\\xed\\xff\\xff\\t\\x00\\x00\\xf8\\xff\\xff\\xf8\\xff\\xff\\xfb\\xff\\xff\\xd4\\xff\\xff\\x08\\x00\\x00\\xe7\\xff\\xff\\x08\\x00\\x00\\x00\\x00\\x00\\x0e\\x00\\x00\\xf1\\xff\\xff\\x07\\x00\\x00\\xf3\\xff\\xff\\x17\\x00\\x00\\xf2\\xff\\xff\\x0c\\x00\\x00\\xfb\\xff\\xff\\xff\\xff\\xff \\x00\\x00\\xdb\\xff\\xff\\x1b\\x00\\x00\\x07\\x00\\x00\\xfe\\xff\\xff\\x0e\\x00\\x00\\t\\x00\\x00\\x15\\x00\\x00\\x1c\\x00\\x00\\x0c\\x00\\x00\\x17\\x00\\x00\\xfd\\xff\\xff\\xfc\\xff\\xff\\x13\\x00\\x00\\t\\x00\\x008\\x00\\x00\\t\\x00\\x006\\x00\\x00 \\x00\\x00,\\x00\\x00H\\x00\\x00\\xfb\\xff\\xff7\\x00\\x00\\xf3\\xff\\xff\\x0c\\x00\\x00\\x17\\x00\\x00\\xe4\\xff\\xff\\x0f\\x00\\x00\\x06\\x00\\x00\\x0c\\x00\\x00\\xf5\\xff\\xff\\xf2\\xff\\xff\\x07\\x00\\x00\\x0e\\x00\\x00\\n\\x00\\x00\\x07\\x00\\x00!\\x00\\x00\\xed\\xff\\xff\\x0c\\x00\\x00\\xed\\xff\\xff\\xf4\\xff\\xff\\xf0\\xff\\xff\\xdb\\xff\\xff\\x0b\\x00\\x00\\xf6\\xff\\xff\\x1f\\x00\\x00\\x03\\x00\\x00\\x1f\\x00\\x00\\x0c\\x00\\x00\\r\\x00\\x00\\x1a\\x00\\x00\\x1d\\x00\\x00/\\x00\\x000\\x00\\x002\\x00\\x00#\\x00\\x00\\t\\x00\\x00\\r\\x00\\x00\\x06\\x00\\x00\\xea\\xff\\xff\\n\\x00\\x00\\x08\\x00\\x00\\x04\\x00\\x00\\x1b\\x00\\x00\\xef\\xff\\xff\\x1a\\x00\\x00\\x19\\x00\\x00\\x1c\\x00\\x00\\n\\x00\\x00\\xf4\\xff\\xff\\xfb\\xff\\xff\\xe7\\xff\\xff\\xf0\\xff\\xff\\xfc\\xff\\xff\\xde\\xff\\xff\\x11\\x00\\x00\\xfc\\xff\\xff\\x15\\x00\\x00\\x01\\x00\\x00\\xfc\\xff\\xff\\xf0\\xff\\xff\\xf5\\xff\\xff\\x1a\\x00\\x00\\xe7\\xff\\xff\\x1a\\x00\\x00\\r\\x00\\x00\\x03\\x00\\x00\\xfd\\xff\\xff\\xf2\\xff\\xff\\x0b\\x00\\x00\\xfa\\xff\\xff\\xf2\\xff\\xff\\xfc\\xff\\xff \\x00\\x00\\r\\x00\\x00$\\x00\\x00\\x0b\\x00\\x00/\\x00\\x00\\xf9\\xff\\xff\\x15\\x00\\x00\\x08\\x00\\x00\\x00\\x00\\x00\\xf8\\xff\\xff\\xff\\xff\\xff\\x06\\x00\\x00\\x12\\x00\\x00\\x0f\\x00\\x00\\x0b\\x00\\x00\\x10\\x00\\x00\\xed\\xff\\xff\\n\\x00\\x00\\xec\\xff\\xff\\xea\\xff\\xff\\xd9\\xff\\xff\\xd1\\xff\\xff\\xe4\\xff\\xff\\xe9\\xff\\xff\\xee\\xff\\xff\\xe9\\xff\\xff\\t\\x00\\x00\\xfa\\xff\\xff\\n\\x00\\x00\\xdf\\xff\\xff\\x08\\x00\\x00\\xfe\\xff\\xff\\xdb\\xff\\xff\\'\\x00\\x00\\xf0\\xff\\xff\\x04\\x00\\x00\\x1b\\x00\\x00\\xe6\\xff\\xff\\xf5\\xff\\xff\\x07\\x00\\x00\\xf5\\xff\\xff\\x12\\x00\\x00\\xf6\\xff\\xff\\x11\\x00\\x00\\x13\\x00\\x00\\x1b\\x00\\x00\\x10\\x00\\x00\\x10\\x00\\x00\\xed\\xff\\xff\\x0c\\x00\\x00\\xf6\\xff\\xff\\x17\\x00\\x00\\xff\\xff\\xff\\xf0\\xff\\xff\\xf9\\xff\\xff\\xdc\\xff\\xff\\x0f\\x00\\x00\\xff\\xff\\xff\\x14\\x00\\x00\\x15\\x00\\x00\\x01\\x00\\x00\\n\\x00\\x00\\x0c\\x00\\x00\\x01\\x00\\x00\\xe6\\xff\\xff\\xe9\\xff\\xff\\xdc\\xff\\xff\\x02\\x00\\x00\\xec\\xff\\xff\\x1a\\x00\\x00\\xf2\\xff\\xff\\x19\\x00\\x00\\x06\\x00\\x00\\x1d\\x00\\x00\\n\\x00\\x00\\x05\\x00\\x00\\x02\\x00\\x00\\x0f\\x00\\x00\\xdd\\xff\\xff\\x13\\x00\\x00\\xce\\xff\\xff\\xfd\\xff\\xff\\xdc\\xff\\xff\\x01\\x00\\x00\\xe5\\xff\\xff\\xec\\xff\\xff\\xfb\\xff\\xff\\xde\\xff\\xff\\x14\\x00\\x00\\xde\\xff\\xff#\\x00\\x00\\xf1\\xff\\xff\\x1c\\x00\\x00\\xf0\\xff\\xff\\r\\x00\\x00\\x11\\x00\\x00\\xfd\\xff\\xff\\xfd\\xff\\xff\\xda\\xff\\xff\\xf8\\xff\\xff\\xde\\xff\\xff\\x11\\x00\\x00\\r\\x00\\x00\\xf1\\xff\\xff\\n\\x00\\x00\\xe9\\xff\\xff\\x11\\x00\\x00\\xe7\\xff\\xff\\xf8\\xff\\xff\\xee\\xff\\xff\\xff\\xff\\xff\\xf1\\xff\\xff\\x01\\x00\\x00\\xf1\\xff\\xff\\x19\\x00\\x00\\n\\x00\\x00\\x10\\x00\\x00\\n\\x00\\x00\\x0b\\x00\\x00\\x15\\x00\\x00\\xe4\\xff\\xff\\x04\\x00\\x00\\xc1\\xff\\xff\\x0b\\x00\\x00\\xeb\\xff\\xff\\n\\x00\\x00\\xf3\\xff\\xff\\xfa\\xff\\xff\\x10\\x00\\x00\\x15\\x00\\x00\\x02\\x00\\x00\\x1a\\x00\\x00\\xd7\\xff\\xff\\r\\x00\\x00\\xe3\\xff\\xff\\x04\\x00\\x00\\xf7\\xff\\xff\\xe9\\xff\\xff\\xeb\\xff\\xff\\xf5\\xff\\xff\\xf4\\xff\\xff\\xf1\\xff\\xff\\x04\\x00\\x00\\xf1\\xff\\xff\\x1e\\x00\\x00\\xfb\\xff\\xff#\\x00\\x00\\xe3\\xff\\xff\\xfd\\xff\\xff\\x16\\x00\\x00\\xf7\\xff\\xff\\r\\x00\\x00\\xe9\\xff\\xff.\\x00\\x00\\x01\\x00\\x00\\x18\\x00\\x00\\x1d\\x00\\x00\\xef\\xff\\xff\\x02\\x00\\x00\\r\\x00\\x00\\xf9\\xff\\xff\\x05\\x00\\x00\\xfa\\xff\\xff\\xf9\\xff\\xff\\xf9\\xff\\xff\\xfe\\xff\\xff\\xeb\\xff\\xff\\x07\\x00\\x00\\xbe\\xff\\xff\\xfc\\xff\\xff\\xef\\xff\\xff\\xeb\\xff\\xff\\x0b\\x00\\x00\\x0f\\x00\\x00\\xf8\\xff\\xff\\x19\\x00\\x00\\x02\\x00\\x00\\x11\\x00\\x00\\xff\\xff\\xff\\x00\\x00\\x00\\x1e\\x00\\x00\\x08\\x00\\x00(\\x00\\x00\\xfc\\xff\\xff\\x06\\x00\\x00\\xe9\\xff\\xff\\xf4\\xff\\xff\\x01\\x00\\x00\\xf1\\xff\\xff\\x17\\x00\\x00\\x00\\x00\\x00\\x08\\x00\\x00\\x0f\\x00\\x00\\x07\\x00\\x00\\xfe\\xff\\xff\\x00\\x00\\x00\\xfe\\xff\\xff\\x0b\\x00\\x00\\xfa\\xff\\xff\\xfd\\xff\\xff\\xf9\\xff\\xff\\xee\\xff\\xff\\xf5\\xff\\xff\\x06\\x00\\x00\\xf2\\xff\\xff\\xf3\\xff\\xff\\xfc\\xff\\xff\\x01\\x00\\x00\\x02\\x00\\x00\\x01\\x00\\x00\\x04\\x00\\x00\\x07\\x00\\x00\\x15\\x00\\x00\\x03\\x00\\x00\\r\\x00\\x00\\xfa\\xff\\xff\\x08\\x00\\x00\\xf0\\xff\\xff\\xfd\\xff\\xff\\xfd\\xff\\xff\\x00\\x00\\x00\\xec\\xff\\xff\\xff\\xff\\xff\\xf8\\xff\\xff\\x0e\\x00\\x00\\x08\\x00\\x00\\xf6\\xff\\xff\\xdf\\xff\\xff\\xf6\\xff\\xff\\xe1\\xff\\xff\\xe8\\xff\\xff\\xe5\\xff\\xff\\x16\\x00\\x00\\x12\\x00\\x00\\xfa\\xff\\xff\\xfc\\xff\\xff\\x06\\x00\\x00\\xeb\\xff\\xff\\x12\\x00\\x00\\xe3\\xff\\xff\\x08\\x00\\x00\\xf9\\xff\\xff)\\x00\\x00\\x07\\x00\\x00\\x17\\x00\\x00\\x0c\\x00\\x00\\x19\\x00\\x00\\x13\\x00\\x00\\t\\x00\\x00\\n\\x00\\x00\\x13\\x00\\x00\\x10\\x00\\x00\\x00\\x00\\x00\\r\\x00\\x00\\x07\\x00\\x00\\x19\\x00\\x00\\r\\x00\\x00*\\x00\\x00\\x07\\x00\\x002\\x00\\x00\\x0f\\x00\\x00\\x16\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xfe\\xff\\xff\\x12\\x00\\x00\\x03\\x00\\x00\\x04\\x00\\x00\\xf8\\xff\\xff\\xf7\\xff\\xff\\xfb\\xff\\xff\\x04\\x00\\x00\\x02\\x00\\x00\\n\\x00\\x00\\x03\\x00\\x00\\xfa\\xff\\xff\\x08\\x00\\x00\\xea\\xff\\xff\\x04\\x00\\x00\\xeb\\xff\\xff\\n\\x00\\x00\\xe8\\xff\\xff\\x14\\x00\\x00\\xf6\\xff\\xff\\x0b\\x00\\x00\\xf3\\xff\\xff\\x04\\x00\\x00\\xfc\\xff\\xff\\x0b\\x00\\x00\\x13\\x00\\x00\\x01\\x00\\x00\\x08\\x00\\x00\\x11\\x00\\x00\\x15\\x00\\x00\\xfd\\xff\\xff\\x19\\x00\\x00\\x04\\x00\\x005\\x00\\x00\\xf2\\xff\\xff\\xe3\\x00\\x00\\x9e\\xff\\xffM\\x03\\x00\\xab\\xfd\\xff5\\x05\\x00e\\xfb\\xff\\x98\\x03\\x00\\xbd\\xfa\\xff\\xcf\\xfe\\xff\\xf5\\xfc\\xff\\xb3\\xf9\\xffA\\x00\\x00\\xbf\\xf8\\xff\\xb7\\x01\\x00\\xa7\\xfb\\xffO\\x01\\x00A\\xff\\xffF\\x00\\x00\\xa5\\x00\\x00E\\xff\\xff\\xca\\x00\\x00W\\xfd\\xff1\\x00\\x00\\x03\\xfb\\xff\\xde\\xfe\\xff\\xe1\\xfa\\xffh\\xfe\\xff_\\xfe\\xff\\xf4\\xfe\\xff\\xce\\x01\\x00#\\x00\\x00.\\x02\\x00\\xa8\\x00\\x00\\x1d\\x00\\x00\\x05\\x00\\x00\\x10\\xfe\\xff\\xc9\\xfe\\xff\\xe8\\xfe\\xff\\xff\\xfe\\xffR\\x01\\x00\\x88\\x00\\x004\\x02\\x00\\xc0\\x00\\x00y\\x00\\x00\\xdb\\xff\\xffA\\xfe\\xff\\xbe\\xfe\\xff\\r\\xfe\\xffQ\\xff\\xff\\xe1\\xff\\xff\\x03\\x00\\x00\\xb9\\x00\\x00y\\x00\\x00\\xb7\\xff\\xffm\\x00\\x00\\xfc\\xfd\\xff\\x88\\x00\\x00\\x1d\\xfe\\xff\\x8b\\x00\\x002\\x00\\x00\\x8a\\xff\\xff\\x97\\x01\\x00\\x0c\\xff\\xff\\xfa\\x00\\x00\\x0b\\xff\\xff\\xde\\xff\\xffc\\xff\\xff0\\xff\\xff\\xe0\\xff\\xff\\x00\\x00\\x00\\xf6\\xff\\xff\\x14\\x01\\x00\\x96\\xff\\xff~\\x01\\x00<\\xff\\xffI\\x01\\x00\\xba\\xff\\xff\\xe5\\x00\\x00y\\x00\\x00\\x19\\x01\\x00\\x9b\\x00\\x00\\xaa\\x00\\x00y\\x00\\x00m\\x00\\x00X\\x00\\x00\\xd3\\xff\\xff\\x9c\\x00\\x00\\x89\\xff\\xff\\x04\\x01\\x00v\\xff\\xff\\xac\\x00\\x00n\\xff\\xff \\x00\\x00)\\xff\\xff\\xb2\\xff\\xff>\\xff\\xff\\xa0\\xff\\xff\\xd2\\xff\\xffB\\xff\\xff\\x90\\x00\\x00W\\xff\\xff\\xd6\\x00\\x00B\\xff\\xff\\xde\\x00\\x00\\xcc\\xff\\xff\\x8a\\x00\\x00\\n\\x00\\x00h\\x00\\x00\\xf2\\xff\\xff\\r\\x00\\x00\\xd4\\xff\\xff\\xc0\\xff\\xff\\xdb\\xff\\xff\\xc3\\xff\\xff*\\x00\\x00\\xf0\\xff\\xff\\x0c\\x00\\x00 \\x00\\x00\\xc4\\xff\\xff\\x0f\\x00\\x00\\xca\\xff\\xff\\xd4\\xff\\xff\\x0c\\x00\\x00\\xf1\\xff\\xff1\\x00\\x00\\xf5\\xff\\xff;\\x00\\x00`\\x00\\x00\\x07\\x00\\x00j\\x00\\x00\\xea\\xff\\xffV\\x00\\x00\\xfe\\xff\\xff\\x05\\x00\\x00\\n\\x00\\x00\\n\\x00\\x00\\x0e\\x00\\x00\"\\x00\\x00\\'\\x00\\x00.\\x00\\x004\\x00\\x00\\x13\\x00\\x00K\\x00\\x00\\xee\\xff\\xff@\\x00\\x00\\xf6\\xff\\xff+\\x00\\x00\\x13\\x00\\x00\\n\\x00\\x00b\\x00\\x00\\xfa\\xff\\xffZ\\x00\\x00\\xe2\\xff\\xff\\x17\\x00\\x00\\xdf\\xff\\xff\\xe2\\xff\\xff\\xf1\\xff\\xff\\xe3\\xff\\xff\\xf5\\xff\\xff\\xf9\\xff\\xff\\xf8\\xff\\xff\\x1b\\x00\\x00%\\x00\\x004\\x00\\x00\\x1f\\x00\\x00H\\x00\\x00&\\x00\\x00A\\x00\\x00:\\x00\\x00V\\x00\\x00\\'\\x00\\x00Y\\x00\\x00\\x03\\x00\\x00-\\x00\\x00\\xd7\\xff\\xff2\\x00\\x00\\xcd\\xff\\xff\\x11\\x00\\x00\\xda\\xff\\xff(\\x00\\x00\\x13\\x00\\x00/\\x00\\x00\\x13\\x00\\x00=\\x00\\x00\\xfd\\xff\\xffE\\x00\\x00\\xf3\\xff\\xffB\\x00\\x00\\xe3\\xff\\xff7\\x00\\x00\\xdd\\xff\\xff\\'\\x00\\x00\\xf9\\xff\\xff\\xf8\\xff\\xff\\x08\\x00\\x00\\x01\\x00\\x00\\x1e\\x00\\x00\\'\\x00\\x007\\x00\\x003\\x00\\x00&\\x00\\x00\\x05\\x00\\x00&\\x00\\x00\\xfc\\xff\\xff\\x02\\x00\\x00\\xf6\\xff\\xff(\\x00\\x00%\\x00\\x00-\\x00\\x00 \\x00\\x00\\x1d\\x00\\x00\\x0b\\x00\\x00*\\x00\\x00\\xea\\xff\\xff%\\x00\\x00\\xe0\\xff\\xff\\x1f\\x00\\x00+\\x00\\x00\\r\\x00\\x00J\\x00\\x00\\x18\\x00\\x00\\'\\x00\\x00\\x17\\x00\\x00\\r\\x00\\x00\\x11\\x00\\x00\\xfb\\xff\\xff\\x19\\x00\\x00\\xef\\xff\\xff)\\x00\\x00\\xdb\\xff\\xff\\x1f\\x00\\x00\\xdd\\xff\\xff\\xfa\\xff\\xff\\x12\\x00\\x00\\xda\\xff\\xff\\x10\\x00\\x00\\xd7\\xff\\xff/\\x00\\x00\\xf6\\xff\\xff\\x0e\\x00\\x00\\xfe\\xff\\xff\\xf8\\xff\\xff\\x12\\x00\\x00\\xfc\\xff\\xff\\x1b\\x00\\x00\\x04\\x00\\x00\\x1f\\x00\\x00\\x1b\\x00\\x00\\x13\\x00\\x00\\'\\x00\\x00\\xfa\\xff\\xff\\t\\x00\\x00\\x05\\x00\\x00\\x14\\x00\\x00&\\x00\\x00\\x12\\x00\\x00/\\x00\\x00\\x1d\\x00\\x009\\x00\\x00\\x00\\x00\\x00\\x12\\x00\\x00\\x01\\x00\\x00\\x12\\x00\\x00\\x11\\x00\\x00\\x18\\x00\\x00\\xfb\\xff\\xff\\x04\\x00\\x00\\xfa\\xff\\xff\\xea\\xff\\xff\\xff\\xff\\xff\\xe2\\xff\\xff\\xfb\\xff\\xff\\x00\\x00\\x00\\xf3\\xff\\xff#\\x00\\x00\\x17\\x00\\x00%\\x00\\x00\\xf5\\xff\\xff\\x19\\x00\\x00\\xe6\\xff\\xff\\x05\\x00\\x00\\xfd\\xff\\xff\\x00\\x00\\x00*\\x00\\x00\\x08\\x00\\x00+\\x00\\x00\\x02\\x00\\x00\\x14\\x00\\x00\\x16\\x00\\x00\\xfd\\xff\\xff\\x1a\\x00\\x00\\xea\\xff\\xff!\\x00\\x00\\x00\\x00\\x00\\x0e\\x00\\x00\\x1d\\x00\\x00\\xd8\\xff\\xff$\\x00\\x00\\xf3\\xff\\xff\\x1a\\x00\\x00\\xeb\\xff\\xff\\x1b\\x00\\x00\\xf1\\xff\\xff\\x12\\x00\\x00\\r\\x00\\x00\\x0f\\x00\\x00\\x17\\x00\\x00\\x03\\x00\\x00\\x08\\x00\\x00\\xf7\\xff\\xff\\x1e\\x00\\x00\\x00\\x00\\x00)\\x00\\x00\\x02\\x00\\x00K\\x00\\x00!\\x00\\x00,\\x00\\x00&\\x00\\x00!\\x00\\x00\\x1d\\x00\\x00\\xf8\\xff\\xff\\xf9\\xff\\xff']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(data[0][1])\n",
    "print(len(data[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换结果: 354\n",
      "转换结果: 354\n"
     ]
    }
   ],
   "source": [
    "# 示例用法\n",
    "fixed_point_data = np.uint16(22658)  # 用作示例的固定点数据\n",
    "result = int(fixed_point_to_decimal(fixed_point_data))\n",
    "print(\"转换结果:\", result)\n",
    "\n",
    "# 示例用法\n",
    "fixed_point_data = np.uint16(22661)  # 用作示例的固定点数据\n",
    "result = int(fixed_point_to_decimal(fixed_point_data))\n",
    "print(\"转换结果:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_cir(cir_bytes, sts1_fp_data, sts2_fp_data):\n",
    "    # 1024 samples, 6 = (3 bytes * 2) per sample, 1 dummy byte\n",
    "    samples = 1024\n",
    "    bytes_per_sample = 3 * 2    \n",
    "    if (len(cir_bytes)!=samples * bytes_per_sample + 1):\n",
    "        raise BaseException('CIR data incomplete...')\n",
    "\n",
    "    # 从那个结构体里面获取，对应sts1_FpIndex，sts2_FpIndex\n",
    "\n",
    "    sts1_fp = int(fixed_point_to_decimal(sts1_fp_data))\n",
    "    sts2_fp = int(fixed_point_to_decimal(sts2_fp_data))\n",
    "\n",
    "    # sts sts2各取128个samples\n",
    "    span_per_sts = SAMPLES\n",
    "    # 从fp-32到fp-32+128\n",
    "    offset_to_fp = -8\n",
    "\n",
    "    sts1_fp_index = 1 + bytes_per_sample * (sts1_fp + offset_to_fp)\n",
    "    sts2_fp_index = 1 + bytes_per_sample * (sts2_fp + offset_to_fp)\n",
    "\n",
    "    sts1 = cir_bytes[sts1_fp_index:sts1_fp_index+span_per_sts * bytes_per_sample]\n",
    "    sts2 = cir_bytes[sts2_fp_index:sts2_fp_index+span_per_sts * bytes_per_sample]\n",
    "    \n",
    "    sts = sts1 + sts2\n",
    "    return  sts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3588\n",
      "(3588, 64) (3588,)\n",
      "3659\n",
      "(3659, 64) (3659,)\n",
      "3503\n",
      "(3503, 64) (3503,)\n",
      "3527\n",
      "(3527, 64) (3527,)\n",
      "4261\n",
      "(4261, 64) (4261,)\n",
      "3517\n",
      "(3517, 64) (3517,)\n",
      "3507\n",
      "(3507, 64) (3507,)\n",
      "5685\n",
      "(5685, 64) (5685,)\n",
      "3706\n",
      "(3706, 64) (3706,)\n",
      "4505\n",
      "(4505, 64) (4505,)\n",
      "3845\n",
      "(3845, 64) (3845,)\n",
      "3756\n",
      "(3756, 64) (3756,)\n",
      "3737\n",
      "(3737, 64) (3737,)\n",
      "3615\n",
      "(3615, 64) (3615,)\n",
      "3629\n",
      "(3629, 64) (3629,)\n",
      "3674\n",
      "(3674, 64) (3674,)\n",
      "3588\n"
     ]
    }
   ],
   "source": [
    "x = np.empty((0, SAMPLES*2))\n",
    "y = np.empty((0, ))\n",
    "x_set = []\n",
    "y_set = []\n",
    "all_cir = []\n",
    "for k in range(len(data)):\n",
    "    dataset = data[k]\n",
    "    n = len(dataset)\n",
    "    processed_data = []\n",
    "    for i in range(n):\n",
    "        sts1_fp_index = dataset[i][0]\n",
    "        sts2_fp_index = dataset[i][1]\n",
    "        raw_cir = dataset[i][2]\n",
    "        # print(sts1_fp_index, sts2_fp_index, len(raw_cir))\n",
    "        try:\n",
    "            valid_raw_cir = get_valid_cir(raw_cir, sts1_fp_index, sts2_fp_index)\n",
    "            cir = process_cir_data(valid_raw_cir)\n",
    "            processed_data.append(cir)\n",
    "        except:\n",
    "            pass\n",
    "            # print(i)\n",
    "    print(len(processed_data))\n",
    "    all_cir.append(processed_data)\n",
    "    \n",
    "    _x = np.array(processed_data)\n",
    "    _y = np.zeros((_x.shape[0],), dtype=int) if \"NLOS\" in files[k] else np.ones((_x.shape[0],), dtype=int)\n",
    "    print(_x.shape, _y.shape)\n",
    "    x = np.concatenate((x, _x), axis = 0)\n",
    "    y = np.concatenate((y, _y), axis = 0)\n",
    "    x_set.append(_x)\n",
    "    y_set.append(_y)\n",
    "    \n",
    "print(len(all_cir[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61714, 64)\n",
      "(61714,)\n",
      "[  11.  +31.j  -24.  +33.j  -31.  +37.j    6.  +33.j    9.   +7.j\n",
      "   44.   +3.j  168.  +92.j  805. +369.j 1544. +463.j 1581. +225.j\n",
      "  514. +143.j -373. +636.j -708.+1113.j -299. +783.j   86. +119.j\n",
      "  292. -174.j  473. -421.j  662. -595.j  508. -528.j   15. -205.j\n",
      " -446.  +19.j -517.  +97.j -163.  +29.j  212.   -9.j  264.  +54.j\n",
      "  146. +148.j  -43. +214.j -124. +565.j   57. +747.j  101. +669.j\n",
      "   -5. +242.j  -90.  +24.j  -24.  +33.j  -31.  +37.j    6.  +33.j\n",
      "    9.   +7.j   44.   +3.j  168.  +92.j  805. +369.j 1544. +463.j\n",
      " 1581. +225.j  514. +143.j -373. +636.j -708.+1113.j -299. +783.j\n",
      "   86. +119.j  292. -174.j  473. -421.j  662. -595.j  508. -528.j\n",
      "   15. -205.j -446.  +19.j -517.  +97.j -163.  +29.j  212.   -9.j\n",
      "  264.  +54.j  146. +148.j  -43. +214.j -124. +565.j   57. +747.j\n",
      "  101. +669.j   -5. +242.j  -90.  +24.j  195. -346.j]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)\n",
    "cir = x[0]\n",
    "label = y[0]\n",
    "print(cir)\n",
    "print(label)\n",
    "cir_amp = np.abs(cir)\n",
    "# fp1 和 fp2太近了，为什么\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure()\n",
    "# plt.plot(np.arange(len(cir_amp)), cir_amp)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from helper import gen_I_Q_Amp_mat, gen_I_Q_Amp_Angle_mat\n",
    "\n",
    "# 检查是否有可用的GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_data(x, y):\n",
    "    merged_x = np.empty((0, SAMPLES*2))\n",
    "    merged_y = np.empty((0, ))\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        merged_x = np.concatenate((merged_x, x[i]), axis = 0)\n",
    "        merged_y = np.concatenate((merged_y, y[i]), axis = 0)\n",
    "        \n",
    "    return merged_x, merged_y\n",
    "\n",
    "def prepare_train_dataset(x, y, l, dim = 1):\n",
    "    val_idx = [2, 13]\n",
    "    \n",
    "    # 检查是否有可用的GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"using:\", device)\n",
    "    \n",
    "    print(f\"validate with {[l[i] for i in val_idx]}\")\n",
    "\n",
    "    # print(len(x), len(y))\n",
    "\n",
    "    # 3 CIR_data_NLOS_2_2.npz\n",
    "    # 5 CIR_data_LOS_3_2.npz\n",
    "    \n",
    "    # for idx, name in enumerate(l):\n",
    "    #     print(idx, name)\n",
    "        \n",
    "    def normalize_x(x):\n",
    "        x_amp = np.abs(x)\n",
    "        x_span = np.max(x_amp, axis=1) - np.min(x_amp, axis=1)\n",
    "\n",
    "        x_norm = x.T/x_span.T\n",
    "        x_norm = x_norm.T\n",
    "        return x_norm\n",
    "\n",
    "    x_norm = []\n",
    "    for i in x:\n",
    "        x_norm.append(normalize_x(i))\n",
    "        \n",
    "    x = x_norm\n",
    "\n",
    "    x_prior = [data for idx, data in enumerate(x) if idx not in val_idx]\n",
    "    y_prior = [data for idx, data in enumerate(y) if idx not in val_idx]\n",
    "    x_post = [data for idx, data in enumerate(x) if idx in val_idx]\n",
    "    y_post = [data for idx, data in enumerate(y) if idx in val_idx]\n",
    "    x_train, y_train = concatenate_data(x_prior, y_prior)\n",
    "    x_val, y_val = concatenate_data(x_post, y_post)\n",
    "\n",
    "    total_len = len(y)\n",
    "    \n",
    "    if dim==1:\n",
    "        x_train = np.abs(x_train)\n",
    "        x_val = np.abs(x_val)\n",
    "    elif dim==3:\n",
    "        x_train = gen_I_Q_Amp_mat(x_train)\n",
    "        x_val = gen_I_Q_Amp_mat(x_val)\n",
    "    elif dim==4:\n",
    "        x_train = gen_I_Q_Amp_Angle_mat(x_train)\n",
    "        x_val = gen_I_Q_Amp_Angle_mat(x_val)\n",
    "\n",
    "    # Convert numpy arrays to PyTorch tensors (use torch.LongTensor for indices)\n",
    "    x_train_tensor = torch.tensor(x_train, dtype=torch.float32, device=device)  # Change dtype to long\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32, device=device)\n",
    "    x_val_tensor = torch.tensor(x_val, dtype=torch.float32, device=device)  # Change dtype to long\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Create PyTorch DataLoader for training and validation\n",
    "    train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using: cuda\n",
      "validate with ['CIR_data_LOS_2_1.txt', 'CIR_data_NLOS_3_2.txt']\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = prepare_train_dataset(x_set, y_set, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the PyTorch model\n",
    "class PyTorchModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PyTorchModel, self).__init__()\n",
    "        # 定义MLP的层\n",
    "        self.w = 32\n",
    "        self.emb = nn.Sequential(\n",
    "            nn.Linear(1*64, self.w, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.w, self.w//4, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.w//4, 1, bias=False),\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        # 定义前向传播\n",
    "        x = self.emb(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 9.25 KB\n"
     ]
    }
   ],
   "source": [
    "from helper import count_parameters_bytes\n",
    "\n",
    "# Create an instance of the PyTorch model and move it to GPU\n",
    "model = PyTorchModel().to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "count_parameters_bytes(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Train Loss: 0.6918036407274162\n",
      "Epoch [1/100] Validation Loss: 0.6909257295940604, Validation Accuracy: 56.22%\n",
      "Epoch [2/100] Train Loss: 0.6911586757566108\n",
      "Epoch [2/100] Validation Loss: 0.6907570644148758, Validation Accuracy: 55.65%\n",
      "Epoch [3/100] Train Loss: 0.6904860498335658\n",
      "Epoch [3/100] Validation Loss: 0.6905539818108082, Validation Accuracy: 55.41%\n",
      "Epoch [4/100] Train Loss: 0.6898457204848877\n",
      "Epoch [4/100] Validation Loss: 0.6903108534004007, Validation Accuracy: 55.34%\n",
      "Epoch [5/100] Train Loss: 0.689166292373693\n",
      "Epoch [5/100] Validation Loss: 0.6900437261377063, Validation Accuracy: 54.97%\n",
      "Epoch [6/100] Train Loss: 0.6884944019468383\n",
      "Epoch [6/100] Validation Loss: 0.6897602145160947, Validation Accuracy: 54.57%\n",
      "Epoch [7/100] Train Loss: 0.6878339085701757\n",
      "Epoch [7/100] Validation Loss: 0.689462672919035, Validation Accuracy: 53.82%\n",
      "Epoch [8/100] Train Loss: 0.6871365220122371\n",
      "Epoch [8/100] Validation Loss: 0.6891586030168193, Validation Accuracy: 53.15%\n",
      "Epoch [9/100] Train Loss: 0.6864963101977766\n",
      "Epoch [9/100] Validation Loss: 0.6888490175562245, Validation Accuracy: 52.50%\n",
      "Epoch [10/100] Train Loss: 0.6857995510240907\n",
      "Epoch [10/100] Validation Loss: 0.6885286415261882, Validation Accuracy: 51.91%\n",
      "Epoch [11/100] Train Loss: 0.6851196275103567\n",
      "Epoch [11/100] Validation Loss: 0.688210342079401, Validation Accuracy: 51.50%\n",
      "Epoch [12/100] Train Loss: 0.684428615927417\n",
      "Epoch [12/100] Validation Loss: 0.6878955167319093, Validation Accuracy: 51.29%\n",
      "Epoch [13/100] Train Loss: 0.6837308266542556\n",
      "Epoch [13/100] Validation Loss: 0.6875820367463997, Validation Accuracy: 50.84%\n",
      "Epoch [14/100] Train Loss: 0.683047029257379\n",
      "Epoch [14/100] Validation Loss: 0.6872704891221864, Validation Accuracy: 50.63%\n",
      "Epoch [15/100] Train Loss: 0.6823444646331689\n",
      "Epoch [15/100] Validation Loss: 0.6869678704866341, Validation Accuracy: 50.66%\n",
      "Epoch [16/100] Train Loss: 0.6816181168483627\n",
      "Epoch [16/100] Validation Loss: 0.6866772653801101, Validation Accuracy: 50.60%\n",
      "Epoch [17/100] Train Loss: 0.6809290477207729\n",
      "Epoch [17/100] Validation Loss: 0.6863932742604187, Validation Accuracy: 50.87%\n",
      "Epoch [18/100] Train Loss: 0.6802480814049339\n",
      "Epoch [18/100] Validation Loss: 0.6861088222690991, Validation Accuracy: 51.15%\n",
      "Epoch [19/100] Train Loss: 0.6795318014309054\n",
      "Epoch [19/100] Validation Loss: 0.6858335754701069, Validation Accuracy: 51.14%\n",
      "Epoch [20/100] Train Loss: 0.678840136569892\n",
      "Epoch [20/100] Validation Loss: 0.6855586692690849, Validation Accuracy: 51.25%\n",
      "Epoch [21/100] Train Loss: 0.6781030244933358\n",
      "Epoch [21/100] Validation Loss: 0.6852902466697353, Validation Accuracy: 51.26%\n",
      "Epoch [22/100] Train Loss: 0.6773590670257318\n",
      "Epoch [22/100] Validation Loss: 0.6850166874272483, Validation Accuracy: 51.48%\n",
      "Epoch [23/100] Train Loss: 0.6765858999320439\n",
      "Epoch [23/100] Validation Loss: 0.6847439259290695, Validation Accuracy: 51.55%\n",
      "Epoch [24/100] Train Loss: 0.6758615671192455\n",
      "Epoch [24/100] Validation Loss: 0.684465765953064, Validation Accuracy: 51.87%\n",
      "Epoch [25/100] Train Loss: 0.6750300648759623\n",
      "Epoch [25/100] Validation Loss: 0.6841961230550494, Validation Accuracy: 52.09%\n",
      "Epoch [26/100] Train Loss: 0.6743414961063331\n",
      "Epoch [26/100] Validation Loss: 0.6839269122907093, Validation Accuracy: 52.09%\n",
      "Epoch [27/100] Train Loss: 0.6734611269182567\n",
      "Epoch [27/100] Validation Loss: 0.6836635539574283, Validation Accuracy: 52.18%\n",
      "Epoch [28/100] Train Loss: 0.6726929148270877\n",
      "Epoch [28/100] Validation Loss: 0.6833986948643412, Validation Accuracy: 52.51%\n",
      "Epoch [29/100] Train Loss: 0.6718137913342103\n",
      "Epoch [29/100] Validation Loss: 0.6831475187625203, Validation Accuracy: 52.57%\n",
      "Epoch [30/100] Train Loss: 0.6710841157397286\n",
      "Epoch [30/100] Validation Loss: 0.682894983461925, Validation Accuracy: 52.63%\n",
      "Epoch [31/100] Train Loss: 0.6702825224650828\n",
      "Epoch [31/100] Validation Loss: 0.6826430468686989, Validation Accuracy: 52.70%\n",
      "Epoch [32/100] Train Loss: 0.6693409233238434\n",
      "Epoch [32/100] Validation Loss: 0.6824063642748764, Validation Accuracy: 52.84%\n",
      "Epoch [33/100] Train Loss: 0.6685409783898248\n",
      "Epoch [33/100] Validation Loss: 0.682160347700119, Validation Accuracy: 52.98%\n",
      "Epoch [34/100] Train Loss: 0.6677045319621959\n",
      "Epoch [34/100] Validation Loss: 0.6819286452872413, Validation Accuracy: 52.89%\n",
      "Epoch [35/100] Train Loss: 0.6668619922508401\n",
      "Epoch [35/100] Validation Loss: 0.6816983052662441, Validation Accuracy: 53.08%\n",
      "Epoch [36/100] Train Loss: 0.665949852600589\n",
      "Epoch [36/100] Validation Loss: 0.6814689806529454, Validation Accuracy: 53.47%\n",
      "Epoch [37/100] Train Loss: 0.6650764794902444\n",
      "Epoch [37/100] Validation Loss: 0.681239444230284, Validation Accuracy: 53.78%\n",
      "Epoch [38/100] Train Loss: 0.6641374198977227\n",
      "Epoch [38/100] Validation Loss: 0.6810127294489315, Validation Accuracy: 53.82%\n",
      "Epoch [39/100] Train Loss: 0.6632211948166966\n",
      "Epoch [39/100] Validation Loss: 0.6808122364538056, Validation Accuracy: 54.03%\n",
      "Epoch [40/100] Train Loss: 0.6622577166529394\n",
      "Epoch [40/100] Validation Loss: 0.6806089585380894, Validation Accuracy: 54.09%\n",
      "Epoch [41/100] Train Loss: 0.6613332405162918\n",
      "Epoch [41/100] Validation Loss: 0.6804138815828732, Validation Accuracy: 54.27%\n",
      "Epoch [42/100] Train Loss: 0.6603079437511588\n",
      "Epoch [42/100] Validation Loss: 0.6802255713513919, Validation Accuracy: 54.47%\n",
      "Epoch [43/100] Train Loss: 0.6593957132282525\n",
      "Epoch [43/100] Validation Loss: 0.6800409390458039, Validation Accuracy: 54.64%\n",
      "Epoch [44/100] Train Loss: 0.6584876992105042\n",
      "Epoch [44/100] Validation Loss: 0.6798722610941955, Validation Accuracy: 54.78%\n",
      "Epoch [45/100] Train Loss: 0.6574628905892651\n",
      "Epoch [45/100] Validation Loss: 0.6796973352985722, Validation Accuracy: 54.83%\n",
      "Epoch [46/100] Train Loss: 0.6565225907455283\n",
      "Epoch [46/100] Validation Loss: 0.6795429385134152, Validation Accuracy: 55.06%\n",
      "Epoch [47/100] Train Loss: 0.655434223681479\n",
      "Epoch [47/100] Validation Loss: 0.6794124857655593, Validation Accuracy: 55.39%\n",
      "Epoch [48/100] Train Loss: 0.6543087571231208\n",
      "Epoch [48/100] Validation Loss: 0.6792875906186444, Validation Accuracy: 55.49%\n",
      "Epoch [49/100] Train Loss: 0.6533130635561932\n",
      "Epoch [49/100] Validation Loss: 0.6791661653135505, Validation Accuracy: 55.66%\n",
      "Epoch [50/100] Train Loss: 0.6522412103986852\n",
      "Epoch [50/100] Validation Loss: 0.6790660082229546, Validation Accuracy: 55.89%\n",
      "Epoch [51/100] Train Loss: 0.6511839498401526\n",
      "Epoch [51/100] Validation Loss: 0.6789756779159818, Validation Accuracy: 56.00%\n",
      "Epoch [52/100] Train Loss: 0.6501349016012018\n",
      "Epoch [52/100] Validation Loss: 0.6788981599467141, Validation Accuracy: 56.20%\n",
      "Epoch [53/100] Train Loss: 0.6491187953278946\n",
      "Epoch [53/100] Validation Loss: 0.678834758166756, Validation Accuracy: 56.36%\n",
      "Epoch [54/100] Train Loss: 0.6479869120573272\n",
      "Epoch [54/100] Validation Loss: 0.6787968852690288, Validation Accuracy: 56.56%\n",
      "Epoch [55/100] Train Loss: 0.6469363119339775\n",
      "Epoch [55/100] Validation Loss: 0.678761458822659, Validation Accuracy: 56.79%\n",
      "Epoch [56/100] Train Loss: 0.6458959743485239\n",
      "Epoch [56/100] Validation Loss: 0.6787450845752444, Validation Accuracy: 56.81%\n",
      "Epoch [57/100] Train Loss: 0.6445922814571327\n",
      "Epoch [57/100] Validation Loss: 0.6787685677409172, Validation Accuracy: 57.00%\n",
      "Epoch [58/100] Train Loss: 0.6435165930827272\n",
      "Epoch [58/100] Validation Loss: 0.6788180928145137, Validation Accuracy: 57.16%\n",
      "Epoch [59/100] Train Loss: 0.6424008142333958\n",
      "Epoch [59/100] Validation Loss: 0.678880109318665, Validation Accuracy: 57.18%\n",
      "Epoch [60/100] Train Loss: 0.6412804290756967\n",
      "Epoch [60/100] Validation Loss: 0.6789728844804424, Validation Accuracy: 57.26%\n",
      "Epoch [61/100] Train Loss: 0.6400888437967948\n",
      "Epoch [61/100] Validation Loss: 0.6790657048778874, Validation Accuracy: 57.33%\n",
      "Epoch [62/100] Train Loss: 0.6389591485890069\n",
      "Epoch [62/100] Validation Loss: 0.679171834141016, Validation Accuracy: 57.46%\n",
      "Epoch [63/100] Train Loss: 0.6376705000774643\n",
      "Epoch [63/100] Validation Loss: 0.6793059509779725, Validation Accuracy: 57.47%\n",
      "Epoch [64/100] Train Loss: 0.6366014994977509\n",
      "Epoch [64/100] Validation Loss: 0.6794549274657454, Validation Accuracy: 57.52%\n",
      "Epoch [65/100] Train Loss: 0.6354642878232013\n",
      "Epoch [65/100] Validation Loss: 0.6796226538717747, Validation Accuracy: 57.50%\n",
      "Epoch [66/100] Train Loss: 0.6342545307352615\n",
      "Epoch [66/100] Validation Loss: 0.6798124435756888, Validation Accuracy: 57.50%\n",
      "Epoch [67/100] Train Loss: 0.6331012685209583\n",
      "Epoch [67/100] Validation Loss: 0.6800199720476355, Validation Accuracy: 57.49%\n",
      "Epoch [68/100] Train Loss: 0.6317997081706898\n",
      "Epoch [68/100] Validation Loss: 0.6802266069820949, Validation Accuracy: 57.52%\n",
      "Epoch [69/100] Train Loss: 0.6308845417002604\n",
      "Epoch [69/100] Validation Loss: 0.6804664800209659, Validation Accuracy: 57.57%\n",
      "Epoch [70/100] Train Loss: 0.62955056930035\n",
      "Epoch [70/100] Validation Loss: 0.6807372623256275, Validation Accuracy: 57.66%\n",
      "Epoch [71/100] Train Loss: 0.6284219972283276\n",
      "Epoch [71/100] Validation Loss: 0.6810141040810517, Validation Accuracy: 57.74%\n",
      "Epoch [72/100] Train Loss: 0.6273272027036904\n",
      "Epoch [72/100] Validation Loss: 0.6813088586287839, Validation Accuracy: 57.66%\n",
      "Epoch [73/100] Train Loss: 0.6261490609941773\n",
      "Epoch [73/100] Validation Loss: 0.6816240841788905, Validation Accuracy: 57.71%\n",
      "Epoch [74/100] Train Loss: 0.6249309236606893\n",
      "Epoch [74/100] Validation Loss: 0.6819654426404408, Validation Accuracy: 57.67%\n",
      "Epoch [75/100] Train Loss: 0.6237476131675952\n",
      "Epoch [75/100] Validation Loss: 0.6823352053761482, Validation Accuracy: 57.73%\n",
      "Epoch [76/100] Train Loss: 0.6228012543791072\n",
      "Epoch [76/100] Validation Loss: 0.6827147571103913, Validation Accuracy: 57.78%\n",
      "Epoch [77/100] Train Loss: 0.6214134811098737\n",
      "Epoch [77/100] Validation Loss: 0.683125703994717, Validation Accuracy: 57.74%\n",
      "Epoch [78/100] Train Loss: 0.6202876592268709\n",
      "Epoch [78/100] Validation Loss: 0.6835617321942534, Validation Accuracy: 57.95%\n",
      "Epoch [79/100] Train Loss: 0.6193233120497272\n",
      "Epoch [79/100] Validation Loss: 0.6840107531419822, Validation Accuracy: 57.92%\n",
      "Epoch [80/100] Train Loss: 0.6180441890444074\n",
      "Epoch [80/100] Validation Loss: 0.6844601668417454, Validation Accuracy: 57.92%\n",
      "Epoch [81/100] Train Loss: 0.6168260538047594\n",
      "Epoch [81/100] Validation Loss: 0.6849518742944513, Validation Accuracy: 57.92%\n",
      "Epoch [82/100] Train Loss: 0.6158851324395218\n",
      "Epoch [82/100] Validation Loss: 0.6854516129408564, Validation Accuracy: 57.98%\n",
      "Epoch [83/100] Train Loss: 0.6146637340200589\n",
      "Epoch [83/100] Validation Loss: 0.6860074235924652, Validation Accuracy: 57.99%\n",
      "Epoch [84/100] Train Loss: 0.6134544793141251\n",
      "Epoch [84/100] Validation Loss: 0.6865729269172464, Validation Accuracy: 57.98%\n",
      "Epoch [85/100] Train Loss: 0.6124726492710918\n",
      "Epoch [85/100] Validation Loss: 0.6871534186814513, Validation Accuracy: 57.98%\n",
      "Epoch [86/100] Train Loss: 0.6112195839647387\n",
      "Epoch [86/100] Validation Loss: 0.6877800635993481, Validation Accuracy: 58.01%\n",
      "Epoch [87/100] Train Loss: 0.6102568399012787\n",
      "Epoch [87/100] Validation Loss: 0.6884142564875739, Validation Accuracy: 58.04%\n",
      "Epoch [88/100] Train Loss: 0.609116791631913\n",
      "Epoch [88/100] Validation Loss: 0.6890856772661209, Validation Accuracy: 57.98%\n",
      "Epoch [89/100] Train Loss: 0.6077828306783278\n",
      "Epoch [89/100] Validation Loss: 0.6897734813392162, Validation Accuracy: 57.97%\n",
      "Epoch [90/100] Train Loss: 0.6067814958877251\n",
      "Epoch [90/100] Validation Loss: 0.6904771620673793, Validation Accuracy: 57.95%\n",
      "Epoch [91/100] Train Loss: 0.6058548971556947\n",
      "Epoch [91/100] Validation Loss: 0.6912265260304723, Validation Accuracy: 57.95%\n",
      "Epoch [92/100] Train Loss: 0.6046424534616761\n",
      "Epoch [92/100] Validation Loss: 0.6919957830437592, Validation Accuracy: 57.97%\n",
      "Epoch [93/100] Train Loss: 0.6034715213317782\n",
      "Epoch [93/100] Validation Loss: 0.6927767688674586, Validation Accuracy: 58.06%\n",
      "Epoch [94/100] Train Loss: 0.6024727428755101\n",
      "Epoch [94/100] Validation Loss: 0.6936060961868081, Validation Accuracy: 58.04%\n",
      "Epoch [95/100] Train Loss: 0.6014545669460744\n",
      "Epoch [95/100] Validation Loss: 0.6944832567657743, Validation Accuracy: 58.09%\n",
      "Epoch [96/100] Train Loss: 0.600253808205245\n",
      "Epoch [96/100] Validation Loss: 0.6953746862709522, Validation Accuracy: 58.16%\n",
      "Epoch [97/100] Train Loss: 0.5991416646110928\n",
      "Epoch [97/100] Validation Loss: 0.6962997067187514, Validation Accuracy: 58.26%\n",
      "Epoch [98/100] Train Loss: 0.598091752244382\n",
      "Epoch [98/100] Validation Loss: 0.6972280740737915, Validation Accuracy: 58.27%\n",
      "Epoch [99/100] Train Loss: 0.5972490168287826\n",
      "Epoch [99/100] Validation Loss: 0.6981877870857716, Validation Accuracy: 58.37%\n",
      "Epoch [100/100] Train Loss: 0.5959541332107517\n",
      "Epoch [100/100] Validation Loss: 0.6991899838404996, Validation Accuracy: 58.40%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 300\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print the average loss for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] Train Loss: {total_loss/len(train_loader)}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        for batch_x_val, batch_y_val in val_loader:\n",
    "            val_outputs = model(batch_x_val)\n",
    "            val_loss += criterion(val_outputs, batch_y_val.view(-1, 1)).item()\n",
    "            val_predictions = (val_outputs > 0.5).float()\n",
    "            correct_predictions += (val_predictions == batch_y_val.view(-1, 1)).sum().item()\n",
    "            total_samples += batch_y_val.size(0)\n",
    "        val_accuracy = correct_predictions / total_samples\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] Validation Loss: {val_loss/len(val_loader)}, Validation Accuracy: {val_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "torch.save(model.state_dict(), 'weights/cnn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\envs\\Pytorch\\lib\\site-packages\\numpy\\lib\\npyio.py:501: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n"
     ]
    }
   ],
   "source": [
    "w = [i.detach().cpu().numpy() for i in model.parameters()]\n",
    "np.save(\"weights/np.npy\", w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
